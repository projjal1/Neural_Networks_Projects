{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human_Conversation_script_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_BIKzYTfT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "e020ef86-50d1-4317-c57e-eeecf035246d"
      },
      "source": [
        "%pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 421.8MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.2)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 450kB 47.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.9MB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqOiXk5vUcOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ9bRxrxaEUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a2f3eadf-d151-46fb-f2e0-0042f6745140"
      },
      "source": [
        "file_data=tf.keras.utils.get_file('file','https://raw.githubusercontent.com/projjal1/datasets/master/human_chat.txt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/projjal1/datasets/master/human_chat.txt\n",
            "122880/115941 [===============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7H6mx3PaOrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=open(file_data,'rb').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPbO9p67ahVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=text.decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPFQ93N0bVqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2f82ff17-89ae-499a-9906-ab464e53cd55"
      },
      "source": [
        "print(text[:220])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Human 1: Hi!\n",
            "Human 2: What is your favorite holiday?\n",
            "Human 1: one where I get to meet lots of different people.\n",
            "Human 2: What was the most number of people you have ever met during a holiday?\n",
            "Human 1: Hard to keep a coun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yD21AHjbj2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now building a vocab of words \n",
        "vocab=sorted(set(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBWamAK6bprV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f120ef35-5e2f-45dd-d126-810cae31eba4"
      },
      "source": [
        "#no. of chars in vocab\n",
        "print(len(vocab))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2XdUzbqbvTo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "49e4e95c-b297-4610-a0c7-94f6bce2924b"
      },
      "source": [
        "#lets hash these characters \n",
        "mapping={v:u for u,v in enumerate(vocab)}\n",
        "print(mapping)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, '+': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '9': 23, ':': 24, ';': 25, '<': 26, '>': 27, '?': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '\\\\': 55, '_': 56, 'a': 57, 'b': 58, 'c': 59, 'd': 60, 'e': 61, 'f': 62, 'g': 63, 'h': 64, 'i': 65, 'j': 66, 'k': 67, 'l': 68, 'm': 69, 'n': 70, 'o': 71, 'p': 72, 'q': 73, 'r': 74, 's': 75, 't': 76, 'u': 77, 'v': 78, 'w': 79, 'x': 80, 'y': 81, 'z': 82, '~': 83, '√©': 84, '‚Äô': 85, '‚Äú': 86, '‚Äù': 87, '‚Ä¶': 88, 'Êπò': 89, 'Áïô': 90, 'Ôºâ': 91, 'Ôºö': 92, 'üòÄ': 93, 'üòÇ': 94, 'üòÜ': 95, 'üòâ': 96, 'üòê': 97, 'üòõ': 98, 'üòû': 99, 'üôÇ': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTOxhs8GcFSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0eea1846-2c8e-4c3a-fb25-5485a7cf682f"
      },
      "source": [
        "#character array\n",
        "char_arr=np.array(vocab)\n",
        "print(char_arr)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '\"' '%' '&' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0' '1' '2'\n",
            " '3' '4' '5' '6' '7' '9' ':' ';' '<' '>' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G'\n",
            " 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y'\n",
            " 'Z' '\\\\' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '~' '√©' '‚Äô' '‚Äú' '‚Äù' '‚Ä¶' 'Êπò'\n",
            " 'Áïô' 'Ôºâ' 'Ôºö' 'üòÄ' 'üòÇ' 'üòÜ' 'üòâ' 'üòê' 'üòõ' 'üòû' 'üôÇ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxScqHJ9dOtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now encoding our text \n",
        "text_as_int=np.array([mapping[each] for each in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKmme2w1dIvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#let's convert text to tensor slices\n",
        "\n",
        "sequence_len=100\n",
        "ex_per_epoch=len(text)//(sequence_len+1)\n",
        "\n",
        "char_ds=tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlrz0zokdgrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences=char_ds.batch(sequence_len+1,drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXDQKRS4dqjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating dataset by splitting them into chunks with both space for input and target\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JVP3_obdzx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bed3219a-53db-4082-c620-2b3d828a185b"
      },
      "source": [
        "print(dataset)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KClLx2Zbd8yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "0cb7dd47-b4a5-4718-ada7-78b156fd6ca7"
      },
      "source": [
        "for each in dataset.take(2):\n",
        "  print(each)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([36, 77, 69, 57, 70,  1, 16, 24,  1, 36, 65,  2,  0, 36, 77, 69, 57,\n",
            "       70,  1, 17, 24,  1, 51, 64, 57, 76,  1, 65, 75,  1, 81, 71, 77, 74,\n",
            "        1, 62, 57, 78, 71, 74, 65, 76, 61,  1, 64, 71, 68, 65, 60, 57, 81,\n",
            "       28,  0, 36, 77, 69, 57, 70,  1, 16, 24,  1, 71, 70, 61,  1, 79, 64,\n",
            "       61, 74, 61,  1, 37,  1, 63, 61, 76,  1, 76, 71,  1, 69, 61, 61, 76,\n",
            "        1, 68, 71, 76, 75,  1, 71, 62,  1, 60, 65, 62, 62, 61, 74])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([77, 69, 57, 70,  1, 16, 24,  1, 36, 65,  2,  0, 36, 77, 69, 57, 70,\n",
            "        1, 17, 24,  1, 51, 64, 57, 76,  1, 65, 75,  1, 81, 71, 77, 74,  1,\n",
            "       62, 57, 78, 71, 74, 65, 76, 61,  1, 64, 71, 68, 65, 60, 57, 81, 28,\n",
            "        0, 36, 77, 69, 57, 70,  1, 16, 24,  1, 71, 70, 61,  1, 79, 64, 61,\n",
            "       74, 61,  1, 37,  1, 63, 61, 76,  1, 76, 71,  1, 69, 61, 61, 76,  1,\n",
            "       68, 71, 76, 75,  1, 71, 62,  1, 60, 65, 62, 62, 61, 74, 61])>)\n",
            "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([70, 76,  1, 72, 61, 71, 72, 68, 61, 13,  0, 36, 77, 69, 57, 70,  1,\n",
            "       17, 24,  1, 51, 64, 57, 76,  1, 79, 57, 75,  1, 76, 64, 61,  1, 69,\n",
            "       71, 75, 76,  1, 70, 77, 69, 58, 61, 74,  1, 71, 62,  1, 72, 61, 71,\n",
            "       72, 68, 61,  1, 81, 71, 77,  1, 64, 57, 78, 61,  1, 61, 78, 61, 74,\n",
            "        1, 69, 61, 76,  1, 60, 77, 74, 65, 70, 63,  1, 57,  1, 64, 71, 68,\n",
            "       65, 60, 57, 81, 28,  0, 36, 77, 69, 57, 70,  1, 16, 24,  1])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
            "array([76,  1, 72, 61, 71, 72, 68, 61, 13,  0, 36, 77, 69, 57, 70,  1, 17,\n",
            "       24,  1, 51, 64, 57, 76,  1, 79, 57, 75,  1, 76, 64, 61,  1, 69, 71,\n",
            "       75, 76,  1, 70, 77, 69, 58, 61, 74,  1, 71, 62,  1, 72, 61, 71, 72,\n",
            "       68, 61,  1, 81, 71, 77,  1, 64, 57, 78, 61,  1, 61, 78, 61, 74,  1,\n",
            "       69, 61, 76,  1, 60, 77, 74, 65, 70, 63,  1, 57,  1, 64, 71, 68, 65,\n",
            "       60, 57, 81, 28,  0, 36, 77, 69, 57, 70,  1, 16, 24,  1, 36])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAOGjxlAeCMG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ff61031b-7073-4be5-f4d5-77457b46cedc"
      },
      "source": [
        "#lets have a look at a sample of our input and output data\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data ',repr(''.join(char_arr[input_example.numpy()])))\n",
        "  print ('Output data ',repr(''.join(char_arr[target_example.numpy()])))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data  'Human 1: Hi!\\nHuman 2: What is your favorite holiday?\\nHuman 1: one where I get to meet lots of differ'\n",
            "Output data  'uman 1: Hi!\\nHuman 2: What is your favorite holiday?\\nHuman 1: one where I get to meet lots of differe'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9G9GrJIeUqz",
        "colab_type": "text"
      },
      "source": [
        "The above example shows the importance of RNN layer. \n",
        "We provide n input and the output takes from 2 index to n+1 index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PZDs77mejyQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f989ee13-cf29-4520-bf22-b8b956d62f44"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ea4GcUeenwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSCiSBYEetV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model specification \n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyew3vtoeuYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model build up \n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QSCV2YMe7ME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "504fa3db-2b92-4f50-8d59-aa5f40f8df1f"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 101) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p4XjjDze2Ip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ff485d45-d900-45a4-b831-42f97a8628c9"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 101)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.6143174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqIrJPCgfDvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compilation of model \n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsDt2zLtfIL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir=\"chkpts\"\n",
        "check_point_pref=os.path.join(checkpoint_dir,'chkpts_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=check_point_pref,save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iytXMzm9fLa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now training our model\n",
        "\n",
        "EPOCHS=40\n",
        "history = model.fit(dataset, epochs=EPOCHS,callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWsevDiPfg7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c2090e0-9571-4ce0-b621-aae8d966a302"
      },
      "source": [
        "#latest checkpoint\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chkpts/chkpts_40'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dsAOm1ofqJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building model from last checkpoint \n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNQQNU7pfynX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "23a4013f-d4d6-4913-b02b-fdda91ca7b0e"
      },
      "source": [
        "#summary of model\n",
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            25856     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 101)            103525    \n",
            "=================================================================\n",
            "Total params: 4,067,685\n",
            "Trainable params: 4,067,685\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDOVDAUkf11i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text generation function \n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 300\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [mapping[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(char_arr[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi7sRNIzgABD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "0a36e455-3e9b-42d6-8d39-ec976472ba23"
      },
      "source": [
        "#testing phase \n",
        "\n",
        "while(True):\n",
        "  print('enter text or type quit to exit')\n",
        "  feed=input()\n",
        "  if feed=='quit':\n",
        "    break\n",
        "  else:\n",
        "    get_string=u\"Human 1: \"+feed+\"\\nHuman 2: \"\n",
        "  output=generate_text(model, start_string=get_string).splitlines()\n",
        "  for each in output[:2]:\n",
        "    print(each)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter text or type quit to exit\n",
            "hi buddy\n",
            "Human 1: hi buddy\n",
            "Human 2: you at for that?\n",
            "enter text or type quit to exit\n",
            "my pleasure\n",
            "Human 1: my pleasure\n",
            "Human 2: hahaha\n",
            "enter text or type quit to exit\n",
            "weather\n",
            "Human 1: weather\n",
            "Human 2: What's your favorite shorts?\n",
            "enter text or type quit to exit\n",
            "tee\n",
            "Human 1: tee\n",
            "Human 2: I haven't thought it was a bunch of fun. what do you loved proformer. Why do Heave it had a pain. It's Vaisto doing some balance between the toptions govivally, my daychis world ... so on. like that's kind of an excere that I know too macho lates and we didn't have a feel theor insountalite it undes\n",
            "enter text or type quit to exit\n",
            "how are you?\n",
            "Human 1: how are you?\n",
            "Human 2: I'm doing great haha. Also mimate should be vacation ser someone who is nuch up on 1. Did never seew hiking watched\n",
            "enter text or type quit to exit\n",
            "your name?\n",
            "Human 1: your name?\n",
            "Human 2: I don't know them. It's just see easy to e. Hope you also stirno to meeth and street food. Cakewing But York has by Stien with that fighting since. I have hoternd How is your day?\n",
            "enter text or type quit to exit\n",
            "quit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}