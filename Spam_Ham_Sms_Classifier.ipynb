{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Ham Sms Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyeencrDxLJDWaxKxhC33G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/projjal1/Neural_Networks_Projects/blob/master/Spam_Ham_Sms_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZtXcNC1Rb3I",
        "colab_type": "text"
      },
      "source": [
        "#Building a model to classify Spam and Ham emails\n",
        "This model will use embeddings and tokenizers to build a neural network model that can classify email messages to spam and ham category. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQpBpskTQb2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "545a1d9c-2165-4719-f010-0519609c7032"
      },
      "source": [
        "%pip install tensorflow-gpu"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.29.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3QN9RTvRjNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAN1IbT1SMs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_url='https://raw.githubusercontent.com/projjal1/datasets/master/spam_ham_dataset.txt'\n",
        "file=keras.utils.get_file('spam_ham_dataset.txt',file_url)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-JG0P26SXVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names={'spam':0,'ham':1}\n",
        "idx2_class_names={0:'spam',1:'ham'}"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Mq2vQWS_k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads dataset\n",
        "    \"\"\"\n",
        "    texts, labels = [], []\n",
        "    with open(file) as f:\n",
        "        for line in f:\n",
        "            split = line.split()\n",
        "            labels.append(split[0].strip())\n",
        "            texts.append(' '.join(split[1:]).strip())\n",
        "    return texts, labels"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nbkXlIPTZrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts,labels=load_data()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSnfWQrNTdjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1593069e-1cf2-4ea6-bf7e-fefc2cd6cd6e"
      },
      "source": [
        "for i in range(2):\n",
        "  print(labels[i],texts[i])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "ham Ok lar... Joking wif u oni...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS_4w1i-TmP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now let us tokenize the text\n",
        "tokenizer=keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "#Convert text sequence to integer\n",
        "texts=tokenizer.texts_to_sequences(texts)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT1EKqmJUNkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebfae982-a885-4d84-e8df-ee0717af8982"
      },
      "source": [
        "print(texts[0])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[49, 471, 4435, 842, 755, 658, 64, 8, 1327, 88, 123, 351, 1328, 148, 2996, 1329, 67, 58, 4436, 144]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PIRHdu3U0KS",
        "colab_type": "text"
      },
      "source": [
        "Now let's define some hyperparameters to develop the model and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fjsprNlUzVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
        "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
        "TEST_SIZE = 0.25 # ratio of testing set\n",
        "BATCH_SIZE=64 #batch size for data"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNnoghpkUP0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now lets convert both the labels and texts to numpy \n",
        "texts=np.array(texts)\n",
        "labels=np.array(labels)\n",
        "\n",
        "#Now we need to pad the texts to make it of uniform size\n",
        "texts=keras.preprocessing.sequence.pad_sequences(texts,maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvmPcp5fVLfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now lets one-hot encode the labels \n",
        "\n",
        "#First we convert the string labels to integer ids\n",
        "labels=[class_names[x] for x in labels]\n",
        "\n",
        "#Now lets categorize labels\n",
        "labels=keras.utils.to_categorical(labels)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZdbZ-xNVpy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "7f5053e1-66b0-4175-9f6c-27d6f68d70dd"
      },
      "source": [
        "labels[:5]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHAsV6QBVvcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(texts,labels,test_size=TEST_SIZE,random_state=7)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9asGmtweXKV1",
        "colab_type": "text"
      },
      "source": [
        "Here we will be using pretrained weights from models to train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w2wEOGtcyYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IonP9hZLaRzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_vectors(tokenizer, dim=100):\n",
        "    embedding_index = {}\n",
        "    with open(f\"data/glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
        "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vectors = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = vectors\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found will be 0s\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXdRu53oZ5BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(tokenizer, lstm_units):\n",
        "    \"\"\"\n",
        "    Constructs the model,\n",
        "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
        "    \"\"\"\n",
        "    # get the GloVe embedding vectors\n",
        "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Embedding(len(tokenizer.word_index)+1,\n",
        "              EMBEDDING_SIZE,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=False,\n",
        "              input_length=SEQUENCE_LENGTH))\n",
        "\n",
        "    model.add(keras.layers.LSTM(lstm_units, recurrent_dropout=0.2))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "    model.add(keras.layers.Dense(2, activation=\"softmax\"))\n",
        "    # compile as rmsprop optimizer\n",
        "    # aswell as with recall metric\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekehl9znlLi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "1a83187a-e582-4999-b30e-f095811b5fd3"
      },
      "source": [
        "# constructs the model with 128 LSTM units\n",
        "model = get_model(tokenizer=tokenizer, lstm_units=128)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading GloVe: 215006it [00:07, 28434.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          901000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 1,018,506\n",
            "Trainable params: 117,506\n",
            "Non-trainable params: 901,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy4AvydPlO9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "88d900d8-3aaf-4d6f-ff0c-1f4f7d489792"
      },
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=20,\n",
        "          verbose=1)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "66/66 [==============================] - 37s 557ms/step - loss: 0.1696 - accuracy: 0.9373 - val_loss: 0.0955 - val_accuracy: 0.9677\n",
            "Epoch 2/20\n",
            "66/66 [==============================] - 36s 552ms/step - loss: 0.0853 - accuracy: 0.9711 - val_loss: 0.1399 - val_accuracy: 0.9476\n",
            "Epoch 3/20\n",
            "66/66 [==============================] - 36s 541ms/step - loss: 0.0683 - accuracy: 0.9768 - val_loss: 0.0779 - val_accuracy: 0.9720\n",
            "Epoch 4/20\n",
            "66/66 [==============================] - 37s 558ms/step - loss: 0.0569 - accuracy: 0.9840 - val_loss: 0.1451 - val_accuracy: 0.9476\n",
            "Epoch 5/20\n",
            "66/66 [==============================] - 36s 551ms/step - loss: 0.0504 - accuracy: 0.9847 - val_loss: 0.0833 - val_accuracy: 0.9742\n",
            "Epoch 6/20\n",
            "66/66 [==============================] - 37s 562ms/step - loss: 0.0396 - accuracy: 0.9890 - val_loss: 0.0839 - val_accuracy: 0.9749\n",
            "Epoch 7/20\n",
            "66/66 [==============================] - 37s 565ms/step - loss: 0.0378 - accuracy: 0.9890 - val_loss: 0.0601 - val_accuracy: 0.9813\n",
            "Epoch 8/20\n",
            "66/66 [==============================] - 37s 566ms/step - loss: 0.0293 - accuracy: 0.9909 - val_loss: 0.0620 - val_accuracy: 0.9828\n",
            "Epoch 9/20\n",
            "66/66 [==============================] - 37s 562ms/step - loss: 0.0280 - accuracy: 0.9923 - val_loss: 0.0602 - val_accuracy: 0.9821\n",
            "Epoch 10/20\n",
            "66/66 [==============================] - 38s 569ms/step - loss: 0.0219 - accuracy: 0.9947 - val_loss: 0.0662 - val_accuracy: 0.9813\n",
            "Epoch 11/20\n",
            "66/66 [==============================] - 37s 563ms/step - loss: 0.0197 - accuracy: 0.9943 - val_loss: 0.0638 - val_accuracy: 0.9821\n",
            "Epoch 12/20\n",
            "66/66 [==============================] - 38s 572ms/step - loss: 0.0166 - accuracy: 0.9947 - val_loss: 0.0625 - val_accuracy: 0.9821\n",
            "Epoch 13/20\n",
            "66/66 [==============================] - 37s 561ms/step - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.1036 - val_accuracy: 0.9799\n",
            "Epoch 14/20\n",
            "66/66 [==============================] - 37s 562ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.0834 - val_accuracy: 0.9785\n",
            "Epoch 15/20\n",
            "66/66 [==============================] - 36s 543ms/step - loss: 0.0099 - accuracy: 0.9967 - val_loss: 0.1025 - val_accuracy: 0.9785\n",
            "Epoch 16/20\n",
            "66/66 [==============================] - 37s 554ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.0590 - val_accuracy: 0.9857\n",
            "Epoch 17/20\n",
            "66/66 [==============================] - 37s 557ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.0734 - val_accuracy: 0.9842\n",
            "Epoch 18/20\n",
            "66/66 [==============================] - 36s 548ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0743 - val_accuracy: 0.9835\n",
            "Epoch 19/20\n",
            "66/66 [==============================] - 36s 544ms/step - loss: 0.0040 - accuracy: 0.9983 - val_loss: 0.1024 - val_accuracy: 0.9813\n",
            "Epoch 20/20\n",
            "66/66 [==============================] - 36s 540ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.0717 - val_accuracy: 0.9828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f36d4bca160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE50JJlTnCmE",
        "colab_type": "text"
      },
      "source": [
        "Now after model is trained we start to predict the labels of input text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXLwpv7pnCWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = model.predict(sequence)[0]\n",
        "    # one-hot encoded vector, revert using np.argmax\n",
        "    return idx2_class_names[np.argmax(prediction)]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA5gjN9ioste",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "182eea56-f20f-4606-a3e5-3cbe70526edf"
      },
      "source": [
        "text = \"Congratulations! you have won 100,000$ this week, click here to claim fast\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6ty1FPNo272",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ebde72d-cce3-486d-cf71-2373d5ba6e08"
      },
      "source": [
        "text = \"â€œCongratulations! Your mobile number has won the sum of $1,000,000 in our Atlantic Mobile Lotto. Contact us via email on [address removed] for claim.\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj2UN5FJpdMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7b84d4e-3ad7-4b66-ee04-09a1bc2d16b6"
      },
      "source": [
        "text = \"Hello I am Maya. Just texted you to keep you updated.\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}