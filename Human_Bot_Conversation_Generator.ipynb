{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Human_Bot_Conversation_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP06hREP84urZJ7OV95ZdoR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Am965t8_pFn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "e0daa135-eb06-41c8-bcbe-b8343fc49d6e"
      },
      "source": [
        "%pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.29.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.4.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6vQm-DXAAqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sly59hfIAmo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file=tf.keras.utils.get_file('master_chat_bot_data.txt','https://raw.githubusercontent.com/projjal1/datasets/master/master_chat_bot_data.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "638W6BbBA1VN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=open(path_to_file,'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwtzZwZtBA1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1adb638c-95b3-4bbc-9364-3ea304e6f8ee"
      },
      "source": [
        "print(\"The length of characters in the text of database\",len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of characters in the text of database 172545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIviY14aBJet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the vocabulary of words\n",
        "vocab=sorted(set(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_KzhKpeBQdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3b8fcc34-166f-4825-c8cc-a828041671b2"
      },
      "source": [
        "print(vocab[:40])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '#', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kKvzwylBWNF",
        "colab_type": "text"
      },
      "source": [
        "#Vectorize the text\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCRNPCWgBTpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a824b7f-708b-4152-cb32-6c82d3baa767"
      },
      "source": [
        "char2idx={u:i for i,u in enumerate(vocab)}\n",
        "print(char2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, '+': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '<': 27, '=': 28, '>': 29, '?': 30, 'A': 31, 'B': 32, 'C': 33, 'D': 34, 'E': 35, 'F': 36, 'G': 37, 'H': 38, 'I': 39, 'J': 40, 'K': 41, 'L': 42, 'M': 43, 'N': 44, 'O': 45, 'P': 46, 'Q': 47, 'R': 48, 'S': 49, 'T': 50, 'U': 51, 'V': 52, 'W': 53, 'X': 54, 'Y': 55, 'Z': 56, '[': 57, ']': 58, '^': 59, '_': 60, 'a': 61, 'b': 62, 'c': 63, 'd': 64, 'e': 65, 'f': 66, 'g': 67, 'h': 68, 'i': 69, 'j': 70, 'k': 71, 'l': 72, 'm': 73, 'n': 74, 'o': 75, 'p': 76, 'q': 77, 'r': 78, 's': 79, 't': 80, 'u': 81, 'v': 82, 'w': 83, 'x': 84, 'y': 85, 'z': 86, '⁇': 87, '？': 88, '🙈': 89}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwa-xaxbDFQG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "146cbc8d-de55-477a-bddb-b5f9efb72328"
      },
      "source": [
        "idx2char=np.array(vocab)\n",
        "print(idx2char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '\"' '#' '%' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0' '1' '2'\n",
            " '3' '4' '5' '6' '7' '8' '9' ':' ';' '<' '=' '>' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' '[' ']' '^' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k'\n",
            " 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '⁇' '？' '🙈']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA_KvDaJDNbm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "935ac64a-cbda-4a44-e917-695d37a17149"
      },
      "source": [
        "#Convert the text to int\n",
        "text_as_int=np.array([char2idx[c] for c in text])\n",
        "print(text_as_int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[38 81 73 ... 74 13  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWPtgIQ_GVpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n0kgsunGYrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7f160803-1b56-4595-da50-a34df48c3f3e"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Human 1: Hi!\\n' ---- characters mapped to int ---- > [38 81 73 61 74  1 16 25  1 38 69  2  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baPZUao7DeVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH8BHziND4wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7FbCLZ1D_el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hA5P3lJEM5P",
        "colab_type": "text"
      },
      "source": [
        "Create test and training batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP25mPHFEE3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "33b1e2d2-de3c-4d22-b4ae-0c044c12150a"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH78RosHESL1",
        "colab_type": "text"
      },
      "source": [
        "Model building using RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ChlEAIEQzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agPsaO8CEYod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YPkqRKOEf8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6HnxTZZFH8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cad5433f-b301-4072-808f-08309ceaefda"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 90) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61rHKwAPEg4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "9e532eda-1645-43c9-ccf6-dca92d9b3197"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           23040     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 90)            92250     \n",
            "=================================================================\n",
            "Total params: 4,053,594\n",
            "Trainable params: 4,053,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r--JvfmyGxtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LuS4KiEEnJm",
        "colab_type": "text"
      },
      "source": [
        "Add an optimizer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogfX3Q02Emy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "977af8e9-78dd-438d-9696-fb57f83d74ea"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 90)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.499277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBOyCrKrE8Vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',loss=loss,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUTikfP9Jkm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrfXme9yFMct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89f72fcf-3079-4102-f39f-3fad747a516c"
      },
      "source": [
        "history=model.fit(dataset, epochs=50,callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1431 - accuracy: 0.9677\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1370 - accuracy: 0.9692\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1316 - accuracy: 0.9709\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1273 - accuracy: 0.9713\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 2s 69ms/step - loss: 0.1298 - accuracy: 0.9711\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1307 - accuracy: 0.9709\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1292 - accuracy: 0.9711\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 2s 69ms/step - loss: 0.1301 - accuracy: 0.9711\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1303 - accuracy: 0.9708\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1294 - accuracy: 0.9712\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1325 - accuracy: 0.9706\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1322 - accuracy: 0.9705\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1307 - accuracy: 0.9706\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1328 - accuracy: 0.9708\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1297 - accuracy: 0.9710\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1302 - accuracy: 0.9708\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1309 - accuracy: 0.9710\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 2s 72ms/step - loss: 0.1301 - accuracy: 0.9713\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1289 - accuracy: 0.9711\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1278 - accuracy: 0.9715\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1274 - accuracy: 0.9715\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1282 - accuracy: 0.9709\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 2s 73ms/step - loss: 0.1278 - accuracy: 0.9714\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1283 - accuracy: 0.9716\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 2s 72ms/step - loss: 0.1254 - accuracy: 0.9718\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1272 - accuracy: 0.9715\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1260 - accuracy: 0.9718\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 2s 74ms/step - loss: 0.1248 - accuracy: 0.9722\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1250 - accuracy: 0.9718\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1243 - accuracy: 0.9721\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 2s 73ms/step - loss: 0.1267 - accuracy: 0.9720\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1254 - accuracy: 0.9719\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1256 - accuracy: 0.9721\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 2s 73ms/step - loss: 0.1252 - accuracy: 0.9715\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1242 - accuracy: 0.9719\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1263 - accuracy: 0.9719\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 0.1269 - accuracy: 0.9712\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 2s 73ms/step - loss: 0.1275 - accuracy: 0.9713\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1290 - accuracy: 0.9714\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1305 - accuracy: 0.9707\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 2s 73ms/step - loss: 0.1291 - accuracy: 0.9710\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 2s 72ms/step - loss: 0.1281 - accuracy: 0.9708\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1284 - accuracy: 0.9711\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1274 - accuracy: 0.9714\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1270 - accuracy: 0.9714\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1250 - accuracy: 0.9719\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1261 - accuracy: 0.9717\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 2s 72ms/step - loss: 0.1247 - accuracy: 0.9724\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 2s 71ms/step - loss: 0.1235 - accuracy: 0.9717\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 2s 73ms/step - loss: 0.1240 - accuracy: 0.9724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxAL2S3AJ0Rv",
        "colab_type": "text"
      },
      "source": [
        "Now we will rebuild the model from the saved weigths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbPYJ3cIJznn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "b2ef324e-984e-4767-dc04-a6b1c1c54424"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_50'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3zalYYbKQF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiwo6ie6JaZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "564d0ec8-6fe9-45be-dddd-c422b8df5ff7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            23040     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 90)             92250     \n",
            "=================================================================\n",
            "Total params: 4,053,594\n",
            "Trainable params: 4,053,594\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "escQ3Qe_IOug",
        "colab_type": "text"
      },
      "source": [
        "#The fun part\n",
        "Let's generate text using the model and predictions on the given dataset nature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NskaNmxTIOSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  output_string=''.join(text_generated)\n",
        "  print(output_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cerlKQhbID3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "b07745e5-e0fb-4799-d964-14a768d0612f"
      },
      "source": [
        "s=input(\"Enter the text: \")\n",
        "s='Human 1: '+s\n",
        "generate_text(model, start_string=s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the text: Where do you live?\n",
            "\n",
            "Human 2: My location in 2: Wow, thank you. You say tell me what's going on?\n",
            "Human 2: I'm just really good at procrastinating! :D I've been putting special?\n",
            "Human 2: That's just they are jumping about in meadows. You love lamb. What do you like to do whendeman 1: About 3 really speak 3olle of the world.\n",
            "Human 2: That's fantastic! :)\n",
            "Human 1: How about you?\n",
            "Human 2: I'm doing pretty good, thanks for asked!\n",
            "Human 2: are they americans?\n",
            "Human 1: How many friends do you have?\n",
            "Human 2: I have thousands of human and robot friends from all over the world. Stay The Beakend to vote which party expraiting to be had.\n",
            "Human 1: Hi!\n",
            "Human 2: Hi it.\n",
            "Human 1: let's speak Check then watch something\n",
            "Human 2: Whatcha gonna eat?\n",
            "Human 1: no idea yet, 1: Which is a more useful subject, philosophy or psychology?\n",
            "Human 2: Psychology, definitely.\n",
            "Human 1: Who are you?\n",
            "Human 2: I am Mitsuku. I want to be your friend.\n",
            "Human 1: Why do you ask?\n",
            "Human 1: how are you today?\n",
            "Human 2: Ah. My logic and cognitive funct\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}