{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7+ICmnok1FeRlBZL4TNr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/projjal1/Neural_Networks_Projects/blob/master/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlEQlmOb8T3Y"
      },
      "source": [
        "%pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOupUOtz8jiW"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3Cv3O3c8qlE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c1848f36-51db-42cb-b4ad-5df01c40ff97"
      },
      "source": [
        "url='https://raw.githubusercontent.com/projjal1/datasets/master/twitter_unprocessed_sentiment.csv'\n",
        "file=tf.keras.utils.get_file('twitter_unprocessed_sentiment.csv',url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/projjal1/datasets/master/twitter_unprocessed_sentiment.csv\n",
            "81526784/81523245 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R6EqDXqrtKV"
      },
      "source": [
        "#Plotting labels \n",
        "idx2class={0:'negative',2:'neutral',4:'positive'}\n",
        "class_names={'negative':0,'neutral':2,'positive':4}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbvp8gePsmyd"
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads dataset\n",
        "    \"\"\"  \n",
        "    df=pd.read_csv(file,encoding='ISO-8859-1')\n",
        "    labels,texts=df['review'],df['content']\n",
        "    return texts,labels\n",
        "\n",
        "text,label=load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C2PB9FZsqTu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "8f10b95f-e604-42d3-cdc7-3c3791e04302"
      },
      "source": [
        "for each in range(4):\n",
        "  print(text[each],label[each])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah! 0\n",
            "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds 0\n",
            "my whole body feels itchy and like its on fire  0\n",
            "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKDnRXJ4ssTT"
      },
      "source": [
        "#Now let us tokenize the text\n",
        "tokenizer=keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "#Convert text sequence to integer\n",
        "text=tokenizer.texts_to_sequences(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOTniU_gufF3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "780cfdad-d5ff-4f5f-b5da-0e1106915c8c"
      },
      "source": [
        "text[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[86, 4, 892]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw57cWrSusss"
      },
      "source": [
        "Let us define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuN6oY2numtT"
      },
      "source": [
        "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
        "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
        "TEST_SIZE = 0.25 # ratio of testing set\n",
        "BATCH_SIZE=64 #batch size for data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_YRC17Quv7b"
      },
      "source": [
        "#Now lets convert both the labels and texts to numpy \n",
        "texts=np.array(text)\n",
        "labels=np.array(label)\n",
        "\n",
        "#Now we need to pad the texts to make it of uniform size\n",
        "texts=keras.preprocessing.sequence.pad_sequences(texts,maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVybR2Fdu0x1"
      },
      "source": [
        "#Now lets one-hot encode the labels \n",
        "\n",
        "#Now lets categorize labels\n",
        "labels=keras.utils.to_categorical(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPBr8kCwvKAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7dfe882-1cd7-4107-a7c1-20cf250d3d0c"
      },
      "source": [
        "labels[6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Jc1QFUvdJd"
      },
      "source": [
        "Now we split the dataset into train and test vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH3u83O_vVwc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(texts,labels,test_size=TEST_SIZE,random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9PG3V6nvZ8m"
      },
      "source": [
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTfw3E-DvhK0"
      },
      "source": [
        "We will load pretrained model's weights and embed that layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsKkiZHSvcxk"
      },
      "source": [
        "def get_embedding_vectors(tokenizer, dim=100):\n",
        "    embedding_index = {}\n",
        "    with open(f\"data/glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
        "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vectors = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = vectors\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found will be 0s\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kESSj3MexC2N"
      },
      "source": [
        "def get_model(tokenizer, lstm_units):\n",
        "    \"\"\"\n",
        "    Constructs the model,\n",
        "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
        "    \"\"\"\n",
        "    # get the GloVe embedding vectors\n",
        "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Embedding(len(tokenizer.word_index)+1,\n",
        "              EMBEDDING_SIZE,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=False,\n",
        "              input_length=SEQUENCE_LENGTH))\n",
        "\n",
        "    model.add(keras.layers.LSTM(lstm_units, recurrent_dropout=0.2))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
        "    # compile as rmsprop optimizer\n",
        "    # aswell as with recall metric\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2fJVmAgxKUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "58fc4ce7-8a48-4f7f-840e-c5a78d774c9c"
      },
      "source": [
        "# constructs the model with 128 LSTM units\n",
        "model = get_model(tokenizer=tokenizer, lstm_units=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading GloVe: 143726it [00:04, 31302.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          48628500  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 48,746,393\n",
            "Trainable params: 117,893\n",
            "Non-trainable params: 48,628,500\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_ASgBSZxN8O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0dc4e0dd-47f2-469d-edb6-2d592303ddfe"
      },
      "source": [
        "model.fit(X_train, Y_train,batch_size=1024, epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "768/768 [==============================] - 353s 460ms/step - loss: 0.4309 - accuracy: 0.8094\n",
            "Epoch 2/2\n",
            "768/768 [==============================] - 350s 456ms/step - loss: 0.3902 - accuracy: 0.8287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5cabc32128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWUcR14L1gAp"
      },
      "source": [
        "def get_predictions(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = keras.preprocessing.sequence.pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = model.predict(sequence)[0]\n",
        "    # one-hot encoded vector, revert using np.argmax\n",
        "    return idx2class[np.argmax(prediction)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m1MCWc31ia0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57612b9e-fb4e-4ede-8881-d153675586a1"
      },
      "source": [
        "text = \"Congratulations! you have won 100,000$ this week, click here to claim fast\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7P_q-yB2Arx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1a07591-bef7-4903-dc77-0df346435e40"
      },
      "source": [
        "text = \"a dear friend of mine commited suicide with a shotgun two years ago\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeHmkRPQ2MB8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08d18368-f87a-4177-b051-3569fbd0125c"
      },
      "source": [
        "text=\"@reda Hello from Texas\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5_N8b802Zmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28910d72-66a2-4c25-b6f3-3beb2d15ad8e"
      },
      "source": [
        "text = \"Trump attempts to clarify 'blood coming out of wherever' remark about Megan Kelly.\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}